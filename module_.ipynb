{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 16:31:18.848643: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-06 16:31:18.884505: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-06 16:31:18.884552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-06 16:31:18.885888: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-06 16:31:18.892421: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-06 16:31:18.893439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-06 16:31:19.966366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[*********************100%%**********************]  50 of 50 completed\n"
     ]
    }
   ],
   "source": [
    "from data_prep_hybrid import hybrid_transformer_database\n",
    "from eq_data_loader import get_eq_data\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  50 of 50 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 16, 20) (501, 16, 20) (1974, 10) (501, 10)\n"
     ]
    }
   ],
   "source": [
    "data_config = json.load(open('data_config.json', 'r'))\n",
    "data = get_eq_data(data_config['correlation_thresh'])\n",
    "xtrain, xtest, ytrain, ytest, train_index, test_index = hybrid_transformer_database(data = data, timestep = 16, lag = 5, lagSD = 5, test_size= 0.2, purge_size = 30)\n",
    "print(xtrain.shape, xtest.shape, ytrain.shape, ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1222, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1184, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'transformer_block/multi_head_self_attention/dense/kernel:0', 'transformer_block/multi_head_self_attention/dense/bias:0', 'transformer_block/multi_head_self_attention/dense_1/kernel:0', 'transformer_block/multi_head_self_attention/dense_1/bias:0', 'transformer_block/multi_head_self_attention/dense_2/kernel:0', 'transformer_block/multi_head_self_attention/dense_2/bias:0', 'transformer_block/multi_head_self_attention/dense_3/kernel:0', 'transformer_block/multi_head_self_attention/dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_block/layer_normalization/gamma:0', 'transformer_block/layer_normalization/beta:0', 'transformer_block/layer_normalization_1/gamma:0', 'transformer_block/layer_normalization_1/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'Output/kernel:0', 'Output/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'lstm/lstm_cell/kernel:0' shape=(20, 64) dtype=float32>), (None, <tf.Variable 'lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>), (None, <tf.Variable 'lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_1/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_1/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_2/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_2/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_3/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_3/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'dense_4/kernel:0' shape=(16, 8) dtype=float32>), (None, <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_5/kernel:0' shape=(8, 16) dtype=float32>), (None, <tf.Variable 'dense_5/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization/gamma:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization/beta:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization_1/gamma:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization_1/beta:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'dense_6/kernel:0' shape=(16, 8) dtype=float32>), (None, <tf.Variable 'dense_6/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'Output/kernel:0' shape=(8, 10) dtype=float32>), (None, <tf.Variable 'Output/bias:0' shape=(10,) dtype=float32>)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m priceData \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(data\u001b[38;5;241m.\u001b[39mloc[train_index, :]\u001b[38;5;241m.\u001b[39mvalues, np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridTransformer_Portfolio(xtrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], xtrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], outputShape \u001b[38;5;241m=\u001b[39m ytrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  headsAttention \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m Dropout, learningRate \u001b[38;5;241m=\u001b[39m LearningRate, priceData \u001b[38;5;241m=\u001b[39m priceData, lb\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m, ub \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocation_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEpochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatchSize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBatchSize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/runtime_test/hybrid_transformer_model.py:136\u001b[0m, in \u001b[0;36mHybridTransformer_Portfolio.allocation_hybrid\u001b[0;34m(self, xtrainRNN, ytrainRNN, Epochs, BatchSize)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTransformer_Model()\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtrainRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mytrainRNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBatchSize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(xtrainRNN)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filectd0ijgx.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1222, in apply_gradients\n        grads_and_vars = self.aggregate_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 1184, in aggregate_gradients\n        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/utils.py\", line 33, in all_reduce_sum_gradients\n        filtered_grads_and_vars = filter_empty_gradients(grads_and_vars)\n    File \"/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/optimizers/utils.py\", line 77, in filter_empty_gradients\n        raise ValueError(\n\n    ValueError: No gradients provided for any variable: (['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'transformer_block/multi_head_self_attention/dense/kernel:0', 'transformer_block/multi_head_self_attention/dense/bias:0', 'transformer_block/multi_head_self_attention/dense_1/kernel:0', 'transformer_block/multi_head_self_attention/dense_1/bias:0', 'transformer_block/multi_head_self_attention/dense_2/kernel:0', 'transformer_block/multi_head_self_attention/dense_2/bias:0', 'transformer_block/multi_head_self_attention/dense_3/kernel:0', 'transformer_block/multi_head_self_attention/dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'transformer_block/layer_normalization/gamma:0', 'transformer_block/layer_normalization/beta:0', 'transformer_block/layer_normalization_1/gamma:0', 'transformer_block/layer_normalization_1/beta:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'Output/kernel:0', 'Output/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'lstm/lstm_cell/kernel:0' shape=(20, 64) dtype=float32>), (None, <tf.Variable 'lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>), (None, <tf.Variable 'lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_1/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_1/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_2/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_2/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_3/kernel:0' shape=(16, 16) dtype=float32>), (None, <tf.Variable 'transformer_block/multi_head_self_attention/dense_3/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'dense_4/kernel:0' shape=(16, 8) dtype=float32>), (None, <tf.Variable 'dense_4/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'dense_5/kernel:0' shape=(8, 16) dtype=float32>), (None, <tf.Variable 'dense_5/bias:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization/gamma:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization/beta:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization_1/gamma:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'transformer_block/layer_normalization_1/beta:0' shape=(16,) dtype=float32>), (None, <tf.Variable 'dense_6/kernel:0' shape=(16, 8) dtype=float32>), (None, <tf.Variable 'dense_6/bias:0' shape=(8,) dtype=float32>), (None, <tf.Variable 'Output/kernel:0' shape=(8, 10) dtype=float32>), (None, <tf.Variable 'Output/bias:0' shape=(10,) dtype=float32>)).\n"
     ]
    }
   ],
   "source": [
    "from hybrid_transformer_model import HybridTransformer_Portfolio\n",
    "\n",
    "Dropout = 0.05\n",
    "LearningRate = 0.01\n",
    "Epochs = 1000\n",
    "Alpha = 0.005\n",
    "DF = 4\n",
    "BatchSize = 16\n",
    "lb = 0\n",
    "ub = 0.3\n",
    "timestep = 16\n",
    "\n",
    "priceData = tf.convert_to_tensor(data.loc[train_index, :].values, np.float32)\n",
    "model = HybridTransformer_Portfolio(xtrain.shape[1], xtrain.shape[2], outputShape = ytrain.shape[1],  headsAttention = 4, dropout= Dropout, learningRate = LearningRate, priceData = priceData, lb= 0.01, ub = 0.2)\n",
    "weights = model.allocation_hybrid(xtrain, ytrain, Epochs = Epochs, BatchSize = BatchSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
